Hadoop es una estructura de software de
código abierto para almacenar datos y
ejecutar aplicaciones en clústeres de
hardware comercial. Proporciona
almacenamiento masivo para cualquier tipo
de datos, enorme poder de procesamiento y
la capacidad de procesar tareas o trabajos
concurrentes virtualmente ilimitados.

¿Por qué es importante Hadoop?
● Capacidad de almacenar y
procesar enormes cantidades de
cualquier tipo de datos, al
instante. Con el incremento
constante de los volúmenes y
variedades de datos, en especial
provenientes de medios sociales y la
Internet de las Cosas (IoT), ésa es
una consideración importante.
● Poder de cómputo. El modelo de
cómputo distribuido de Hadoop
procesa big data a gran velocidad.
Cuantos más nodos de cómputo
utiliza usted, mayor poder de
procesamiento tiene.
● Tolerancia a fallos. El
procesamiento de datos y
aplicaciones está protegido contra
fallos del hardware. Si falla un nodo,
los trabajos son redirigidos
automáticamente a otros modos para
asegurarse de que no falle el
procesamiento distribuido. Se
almacenan múltiples copias de todos
los datos de manera automática.

● Flexibilidad. A diferencia de las
bases de datos relacionales, no tiene
que procesar previamente los datos
antes de almacenarlos. Puede
almacenar tantos datos como desee y
decidir cómo utilizarlos más tarde.
Eso incluye datos no estructurados
como texto, imágenes y videos.
● Bajo costo. La estructura de código
abierto es gratuita y emplea
hardware comercial para almacenar
grandes cantidades de datos.

● Escalabilidad. Puede hacer crecer
fácilmente su sistema para que
procese más datos son sólo agregar
nodos. Se requiere poca
administración.

Historia
A medida que la World Wide Web creció a
finales de los 1900 y principios de los 2000,
se crearon buscadores (o motores de
búsqueda) e índices para ayudar a localizar
información relevante dentro de contenido
basado en texto. En sus primeros años, los
resultados de las búsquedas eran entregados
por humanos. Pero a medida que la Web
creció de docenas a millones de páginas, se
requirió de la automatización. Se crearon los
rastreadores Web, muchos como proyectos
dirigidos por universidades, y entonces se
iniciaron las primeras compañías de
buscadores (Yahoo, AltaVista, etc.).
Uno de estos proyectos fue un buscador
Web de código abierto llamado Nutch – idea
original de Doug Cutting y Mike Cafarella.
Deseaban generar resultados de búsquedas
en la Web a mayor velocidad distribuyendo
datos y cálculos en diferentes computadoras
de modo que se pudieran procesar múltiples
tareas de manera simultánea. Durante este
tiempo, estaba en progreso otro proyecto de
buscador llamado Google. Éste se basaba en
el mismo concepto – almacenar y procesar
datos de manera distribuida y automatizada
de modo que se pudieran generar resultados
de búsquedas en la Web a mayor velocidad.

En 2006, Cutting se unió a Yahoo y se llevó
con él el proyecto Nutch, así como también

ideas basadas en los trabajos iniciales de
Google con la automatización del
almacenaje y procesamiento de datos
distribuidos. El proyecto Nutch fue dividido
– la parte del rastreador Web se mantuvo
como Nutch y la parte de cómputo y
procesamiento distribuido se convirtió en
Hadoop (en honor del elefante de juguete del
hijo de Cutting). En 2008, Yahoo presentó
Hadoop como proyecto de código abierto.
Hoy día, la estructura y el ecosistema de
tecnologías de Hadoop son gestionados y
mantenidos por la Apache Software
Foundation (ASF) sin fines de lucro, que es
una comunidad global de programadores de
software y otros contribuyentes.

Uso en la actualidad
Llegando más allá de su meta original de
realizar búsquedas en millones (o miles de
millones) de páginas Web y producir
resultados relevantes, muchas
organizaciones voltean a ver a Hadoop
como su próxima plataforma del big data.
Entre sus usos populares actuales se
cuentan:
● Almacenaje y archivo de datos de
bajo costo
El costo accesible del hardware
comercial hace que Hadoop sea útil
para almacenar y combinar datos
tales como transacciones, medios
sociales, de sensores, de máquinas,
científicos, secuencias de clics, etc.
El almacenaje de bajo costo le
permite conservar información que
no se considera decisiva en el
momento pero que podría desear
analizar más adelante.

● Caja de arena para descubrimiento
y análisis
Como Hadoop fue diseñado para
sortear grandes volúmenes de datos
en diversas formas, puede ejecutar
algoritmos analíticos. La analítica
del big data en Hadoop puede ayudar
a su organización a operar con
mayor eficiencia, descubrir nuevas
oportunidades y obtener una ventaja
competitiva de siguiente nivel. El
enfoque de la caja de arena ofrece
una oportunidad para innovar con
una mínima inversión.
● Data Lake
Los lagos de datos (data lakes)
permiten almacenar datos en su
formato original exacto. La meta es
ofrecer una vista de los datos cruda o
no refinada a científicos y analistas
de datos para que realicen tareas de
descubrimiento y analítica. Les
ayuda a formular preguntas nuevas o
difíciles sin restricciones. Los data
lakes no son un reemplazo de los
almacenes de datos. De hecho, cómo
proteger y gobernar lagos de datos es
un tema de gran interés para las áreas
de TI. Pueden apoyarse en técnicas
de federación de datos para crear
estructuras de datos lógicas.

● Complemente su almacén de datos
Ahora vemos que Hadoop comienza
a situarse a un lado de los entornos
de almacenes de datos, además de
que ciertos conjuntos de datos se
llevan del almacén de datos a

Hadoop o que datos nuevos se van
directamente a Hadoop. La meta
final para toda organización es tener
una plataforma correcta para
almacenar y procesar datos de
diferentes esquemas, formatos, etc.
para justificar diferentes casos de
uso que se puedan integrar en
diferentes niveles.

● IoT y Hadoop
Las cosas en IoT necesitan saber qué
comunicar y cuándo actuar. En el
centro de IoT hay un torrente de
datos en transición siempre activo.
Hadoop se utiliza a menudo como el
almacén de datos de millones o miles
de millones de transacciones. Las
capacidades masivas de almacenaje
y procesamiento le permiten también
usar Hadoop como caja de arena para
el descubrimiento y la definición de
patrones cuya instrucción
prescriptiva deberá ser monitoreada.
Luego puede mejorar continuamente
estas instrucciones, ya que Hadoop
se actualiza de manera constante con
nuevos datos que no concuerdan con
patrones definidos con anterioridad.



Fuentes:

¿Qué es Hadoop? – Amazon Web
Services (AWS). (s. f.). Amazon Web
Services, Inc. Recuperado 24 de
septiembre de 2022, de
https://aws.amazon.com/es/elasticmapr
educe/details/hadoop/
¿Qué es Hadoop? |. (s. f.). Google
Cloud. Recuperado 24 de septiembre de

2022, de

https://cloud.google.com/learn/what-is-
hadoop?hl=es

Descubre por qué los big data y Hadoop
parecen ser inseparables. (s. f.).
Tableau. Recuperado 24 de septiembre
de 2022, de

https://www.tableau.com/es-
mx/learn/articles/big-data-hadoop-
explained
